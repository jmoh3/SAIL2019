{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas is a python library for data manipulation and analysis\n",
    "import pandas as pd\n",
    "# numpy is a python library that supports multidimensional array and matrix manipulation\n",
    "import numpy as np\n",
    "\n",
    "# Load the data from the csv into a dataframe\n",
    "df = pd.read_csv('word_sentiments.csv')\n",
    "# Drop unnecessary column (need inplace otherwise it doesn't save the changes to df)\n",
    "df.drop('Unnamed: 0', axis='columns', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is</td>\n",
       "      <td>0.125</td>\n",
       "      <td>[0.0070495605, -0.07324219, 0.171875, 0.022583...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>[0.08496094, -0.095214844, 0.119140625, 0.1118...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>will</td>\n",
       "      <td>0.125</td>\n",
       "      <td>[0.048828125, 0.16699219, 0.16894531, 0.087402...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>an</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>[0.12597656, 0.19042969, 0.06982422, 0.0722656...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>had</td>\n",
       "      <td>0.250</td>\n",
       "      <td>[-0.05810547, 0.05810547, 0.013305664, -0.0003...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word  score                                          embedding\n",
       "0    is  0.125  [0.0070495605, -0.07324219, 0.171875, 0.022583...\n",
       "1   not -0.625  [0.08496094, -0.095214844, 0.119140625, 0.1118...\n",
       "2  will  0.125  [0.048828125, 0.16699219, 0.16894531, 0.087402...\n",
       "3    an -0.125  [0.12597656, 0.19042969, 0.06982422, 0.0722656...\n",
       "4   had  0.250  [-0.05810547, 0.05810547, 0.013305664, -0.0003..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is what the first 5 rows of our dataframe looks like\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# load the word embeddings into the model from file\n",
    "filename = 'word2vecSmall.bin.gz'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "\n",
    "# Add column for word embeddings to dataframe\n",
    "# lambda functions are functions without names - what this is essentially saying is to go through\n",
    "# every word in our dataframe and access its word embedding from the model\n",
    "df['embedding'] = df['word'].apply(lambda x: model[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the words we want to test our model with - feel free to change these!\n",
    "# Ask audience to give us these words\n",
    "test_words = ['nice', 'mean', 'bad', 'good', 'sad', 'happy', 'fantastic', 'terrible']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>good</td>\n",
       "      <td>0.500</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>bad</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>[0.06298828, 0.12451172, 0.11328125, 0.0732421...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>happy</td>\n",
       "      <td>0.875</td>\n",
       "      <td>[-0.0005187988, 0.16015625, 0.0016098022, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>Good</td>\n",
       "      <td>0.500</td>\n",
       "      <td>[-0.10888672, -0.07470703, -0.045410156, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>fantastic</td>\n",
       "      <td>0.375</td>\n",
       "      <td>[-0.122558594, -0.037841797, -0.12402344, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>sad</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>[0.18945312, 0.045898438, 0.06689453, -0.04467...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>terrible</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>[0.1640625, 0.19238281, 0.092285156, 0.1308593...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833</th>\n",
       "      <td>Bad</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>[-0.078125, -0.11279297, 0.018676758, 0.080566...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>Happy</td>\n",
       "      <td>0.875</td>\n",
       "      <td>[0.05078125, -0.109375, -0.12597656, 0.1240234...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2638</th>\n",
       "      <td>GOOD</td>\n",
       "      <td>0.500</td>\n",
       "      <td>[-0.34179688, -0.41015625, 0.45117188, -0.2871...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6279</th>\n",
       "      <td>Fantastic</td>\n",
       "      <td>0.375</td>\n",
       "      <td>[-0.064453125, -0.079589844, -0.17675781, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6365</th>\n",
       "      <td>Sad</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>[0.22851562, -0.04663086, 0.11230469, -0.00616...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7451</th>\n",
       "      <td>SAD</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>[-0.017211914, -0.23046875, -0.04345703, 0.062...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8063</th>\n",
       "      <td>Terrible</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>[0.296875, -0.06640625, 0.18457031, 0.00148773...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11202</th>\n",
       "      <td>BAD</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>[-0.11035156, -0.12060547, 0.29101562, 0.13183...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  score                                          embedding\n",
       "16          good  0.500  [0.040527344, 0.0625, -0.017456055, 0.07861328...\n",
       "106          bad -0.875  [0.06298828, 0.12451172, 0.11328125, 0.0732421...\n",
       "164        happy  0.875  [-0.0005187988, 0.16015625, 0.0016098022, 0.02...\n",
       "649         Good  0.500  [-0.10888672, -0.07470703, -0.045410156, -0.00...\n",
       "717    fantastic  0.375  [-0.122558594, -0.037841797, -0.12402344, 0.02...\n",
       "750          sad -0.625  [0.18945312, 0.045898438, 0.06689453, -0.04467...\n",
       "907     terrible -0.625  [0.1640625, 0.19238281, 0.092285156, 0.1308593...\n",
       "1833         Bad -0.875  [-0.078125, -0.11279297, 0.018676758, 0.080566...\n",
       "2288       Happy  0.875  [0.05078125, -0.109375, -0.12597656, 0.1240234...\n",
       "2638        GOOD  0.500  [-0.34179688, -0.41015625, 0.45117188, -0.2871...\n",
       "6279   Fantastic  0.375  [-0.064453125, -0.079589844, -0.17675781, 0.00...\n",
       "6365         Sad -0.625  [0.22851562, -0.04663086, 0.11230469, -0.00616...\n",
       "7451         SAD -0.625  [-0.017211914, -0.23046875, -0.04345703, 0.062...\n",
       "8063    Terrible -0.625  [0.296875, -0.06640625, 0.18457031, 0.00148773...\n",
       "11202        BAD -0.875  [-0.11035156, -0.12060547, 0.29101562, 0.13183..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the data we are testing our model on (contains words in blacklist)\n",
    "# This is selecting all the rows in the dataframe whose rows are in the list of test words above, ignoring\n",
    "# uppercase/lowercase.\n",
    "test_df = df[df['word'].str.lower().str.strip().isin(test_words)].copy()\n",
    "test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'blacklist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3732bdf279ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This is the data we are training our model on (words NOT in blacklist - that's what the tilda does)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblacklist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'blacklist' is not defined"
     ]
    }
   ],
   "source": [
    "# This is the data we are training our model on (words NOT in blacklist - that's what the tilda does)\n",
    "train_df = df[~df['word'].str.lower().str.strip().isin(test_words)].copy()\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures input to LinearRegression model is in acceptable format\n",
    "# stack the word embeddings into a matrix of size (# words, len word embedding)\n",
    "X_train = np.stack(train_df['embedding'])\n",
    "\n",
    "# get sentiment scores of training words\n",
    "Y_train = train_df['score'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.stack(test_df['embedding'])\n",
    "Y_test = test_df['score'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Need to import model from sci-kit-learn, a popular python machine learning library\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "classifier = LinearRegression()\n",
    "\n",
    "# Training step that Jason discussed - find optimal coefficients m1, m2, ... m300\n",
    "# and intercept b for all elements in word embedding to predict the sentiment score.\n",
    "classifier.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the m1, m2, ..., m300 we wanted to find.\n",
    "classifier.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gets the shape of coefficients of the LinearRegression model, which we can see is the same\n",
    "# as the length of the embedding input.\n",
    "classifier.coef_.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the intercept of the LinearRegression model (the 'b' in y = mx + b)\n",
    "classifier.intercept_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the r^2 coefficient - don't need to know nitty gritty of this, it just measures how\n",
    "# accurate our model is.\n",
    "# We want our r^2 to be above 0 (positive correlation between predicted sentiment scores and actual scores)\n",
    "# and as close to 1 as possible (that would mean perfect correlation).\n",
    "\n",
    "# 0.58 is pretty good!\n",
    "print(\"Score:\", classifier.score(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see this in action, let's get a word from our test set that obviously has a positive sentiment and\n",
    "# one with an obvious negative sentiment\n",
    "happy_vector = [np.array(model['happy'])]\n",
    "sad_vector = [np.array(model['sad'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's an example of output from the model - 'happy' should have a higher sentiment score than 'sad'\n",
    "print(classifier.predict(happy_vector))\n",
    "print(classifier.predict(sad_vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# It would help us humans understand what's going on better if we had a way to visualize these predictions.\n",
    "# pandas has some pretty nice built in plotting features - we're going to put the predicted scores\n",
    "# into the dataframe so pandas can plot it for us.\n",
    "#\n",
    "# We can create a new column called predicted_score and set it equal to the classifier's output for the X_test\n",
    "# matrix we made above.\n",
    "test_df['predicted_score'] = classifier.predict(X_test)\n",
    "\n",
    "# To make our plot prettier let's sort by predicted scores, so bars will be displayed in order of \n",
    "# lowest to highest predicted sentiment\n",
    "test_df.sort_values(by=['predicted_score'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now see the new column we've added into the dataframe.\n",
    "test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bar plot showing what our model thinks the sentiments of our test words are and what they actually are\n",
    "# Doesn't get the magnitude of positive or negative sentiment that well, but it gets the general idea!\n",
    "test_df.plot.bar(x='word',y=['predicted_score', 'score'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
